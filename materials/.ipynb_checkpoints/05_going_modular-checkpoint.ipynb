{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going modular with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/going_modular/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/going_modular/data_setup.py\n",
    "\"\"\"\n",
    "containes functionality for creating pytorch dataloaders for image classification data\n",
    "\"\"\"\n",
    "import os \n",
    "\n",
    "from torchvision import datasets, transforms \n",
    "from torch.utils.data import DataLoader \n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "    \"\"\"\n",
    "    creates training and testing DataLoaders. \n",
    "\n",
    "    Takes in a training directory and testing directory path and turns them\n",
    "    into pytorch datasets and then into pytorch dataloaders. \n",
    "\n",
    "    Args:\n",
    "        train_dir: path to training directory. \n",
    "        test_dir: path to testing directory \n",
    "        transform: torchvision transforms to perform on training and testing data. \n",
    "        batch_size: number of samples per batch in each of the dataloaders. \n",
    "        num_workers: an integer for number of workers per dataloader.\n",
    "\n",
    "    returns: \n",
    "        A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "        where class_names is a list of the target classes. \n",
    "\n",
    "        Example usage: \n",
    "            train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=path/to/train_dir, \n",
    "                                                                                test_dir=path/to/test_dir, \n",
    "                                                                                transform=some_transform,\n",
    "                                                                                batch_size=32,\n",
    "                                                                                num_workers=4)                                                                                                                 \n",
    "    \"\"\"\n",
    "\n",
    "    # use ImageFolder to create datasets \n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "    # get class names \n",
    "    class_names = train_data.classes \n",
    "\n",
    "    # turn images into dataloaders \n",
    "    train_dataloader = DataLoader(\n",
    "        train_data, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    test_dataloader = DataLoader(\n",
    "        test_data, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=num_workers, \n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting going_modular/going_modular/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/going_modular/model_builder.py\n",
    "\"\"\"\n",
    "contains pytorch model code to instantiate a TinyVGG model.\n",
    "\"\"\"\n",
    "import torch \n",
    "from torch import nn \n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                     out_channels=hidden_units,\n",
    "                     kernel_size=3, \n",
    "                     stride=1, \n",
    "                     padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                     out_channels=hidden_units,\n",
    "                     kernel_size=3, \n",
    "                     stride=1, \n",
    "                     padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, \n",
    "                        stride=2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0), \n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*13*13, out_features=output_shape)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        z = self.conv_block_1(x)\n",
    "        z = self.conv_block_2(z)\n",
    "        z = self.classifier(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/going_modular/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/going_modular/engine.py\n",
    "\"\"\"\n",
    "contains functions for training and testing a pytorch model\n",
    "\"\"\"\n",
    "import torch \n",
    "\n",
    "from tqdm.auto import tqdm \n",
    "from typing import Dict, List, Tuple \n",
    "\n",
    "def train_step(model: torch.nn.Module, \n",
    "              dataloader: torch.utils.data.DataLoader, \n",
    "              loss_fn: torch.nn.Module, \n",
    "              optimizer: torch.optim.Optimizer, \n",
    "              device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Trains a pytorch model for a single epoch \n",
    "\n",
    "    turns a target model to training mode then runs through all of the required training steps\n",
    "    (forward pass, loss calculation, optimizer step).\n",
    "\n",
    "    Args: \n",
    "        model: pytorch model\n",
    "        dataloader: dataloader insatnce for the model to be trained on \n",
    "        loss_fn: pytorch loss function to calculate loss\n",
    "        optimizer: pytorch optimizer to help minimize the loss function\n",
    "        device: target device\n",
    "\n",
    "    returns:\n",
    "        a tuple of training loss and training accuracy metrics\n",
    "        in the form (train_loss, train_accuracy)\n",
    "    \"\"\"\\\n",
    "    # put the model into training mode\n",
    "    model.train()\n",
    "    \n",
    "    # setup train loss and train accuracy \n",
    "    train_loss, train_accuracy = 0, 0 \n",
    "\n",
    "    # loop through data laoder batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # send data to target device \n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # forward pass \n",
    "        logits = model(X)\n",
    "\n",
    "        # calculate loss and accumulate loss \n",
    "        loss = loss_fn(logits, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        # optimizer zero grad \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # loss backward \n",
    "        loss.backward()\n",
    "\n",
    "        # optimizer step \n",
    "        optimizer.step()\n",
    "\n",
    "        # calculate and accumulate accuracy metric across all batches\n",
    "        preds = torch.softmax(logits, dim=-1).argmax(dim=-1)\n",
    "        train_accuracy += (preds == y).sum().item()/len(pred)\n",
    "\n",
    "    # adjust metrics to get average loss and accuracy per batch \n",
    "    train_loss /= len(dataloader)\n",
    "    train_accuracy /= len(dataloader)\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def test_step(model: torch.nn.Module, \n",
    "             dataloader: torch.utils.data.DataLoader, \n",
    "             loss_fn: torch.nn.Module, \n",
    "             device: torch.device) -> Tuple[float, float]:\n",
    "    \"\"\"Tests a pytorch model for a single epoch\n",
    "\n",
    "    Turns a target model to eval mode and then performs a forward pass on a testing\n",
    "    dataset. \n",
    "\n",
    "    Args: \n",
    "        model: pytorch model\n",
    "        dataloader: dataloader insatnce for the model to be tested on \n",
    "        loss_fn: loss function to calculate loss (errors)\n",
    "        device: target device to compute on \n",
    "\n",
    "    returns:\n",
    "        A tuple of testing loss and testing accuracy metrics.\n",
    "        In the form (test_loss, test_accuracy)\n",
    "    \"\"\"\n",
    "    # put the model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # setup test loss and test accuracy \n",
    "    test_loss, test_accuracy = 0, 0 \n",
    "\n",
    "    # turn on inference mode \n",
    "    with torch.inference_mode():\n",
    "        # loop through all batches \n",
    "        for X, y in dataloader: \n",
    "            # send data to target device\n",
    "            X, y  = X.to(device), y.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            logits = model(X)\n",
    "\n",
    "            # calculate and accumulate loss\n",
    "            loss = loss_fn(logits, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # calculate and accumulate accuracy \n",
    "            test_preds = torch.softmax(logits, dim=-1).argmax(dim=-1)\n",
    "            test_acc += ((test_pred == y).sum().item()/len(test_preds))\n",
    "    # adjust metrics to get average loss and accuracy per batch \n",
    "    test_loss /= len(dataloader)\n",
    "    test_accuracy /= len(dataloader)\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "def train(model: torch.nn.Module, \n",
    "         train_dataloader: torch.utils.data.DataLoader, \n",
    "         test_dataloader: torch.utils.data.DataLoader, \n",
    "         optimizer: torch.optim.Optimizer, \n",
    "         loss_fn: torch.nn.Module, \n",
    "         epochs: int, \n",
    "         device: torch.device) -> Dict[str, List]:\n",
    "    \"\"\"Trains and tests pytorch model\n",
    "\n",
    "    passes a target model through train_step() and test_step() \n",
    "    functions for a number of epochs, training and testing the model in the same epoch loop.\n",
    "\n",
    "    calculates, prints and stores evaluation metric throughout. \n",
    "\n",
    "    Args: \n",
    "        model: pytorch model\n",
    "        train_dataloader: DataLoader instance for the model to be trained on\n",
    "        test_dataloader: DataLoader instance for the model to be tested on\n",
    "        optimizer: pytorch optimizer\n",
    "        loss_fn: pytorch loss function\n",
    "        epochs: integer indicating how many epochs to train for\n",
    "        device: target device to compute on \n",
    "\n",
    "    returns: \n",
    "        A dictionaru of training and testing loss as well as training and testing accuracy \n",
    "        metrics. Each metric has a value in a list for each epoch. \n",
    "\n",
    "        In the form: {train_loss: [...],\n",
    "                      train_acc: [...],\n",
    "                      test_loss: [...],\n",
    "                      test_acc: [...]}\n",
    "    \"\"\"\n",
    "    # create an empty dictionary \n",
    "    results = {\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # loop through training and testing steps for a number of epochs \n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                          dataloader=train_dataloader, \n",
    "                                          loss_fn=loss_fn, \n",
    "                                          optimizer=optimizer, \n",
    "                                          device=device)\n",
    "        test_loss, test_acc = test_step(model=model, \n",
    "                                       dataloader=test_dataloader, \n",
    "                                       loss_fn=loss_fn,\n",
    "                                       device=device)\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1} | \" \n",
    "                f\"train_loss: {train_loss:.4f} | \"\n",
    "                f\"train_acc: {train_acc:.4f\"} | \"\n",
    "                f\"test_loss: {test_loss:.4f} | \"\n",
    "                f\"test_acc: {test_acc:.4f}\"\n",
    "            )\n",
    "\n",
    "        # update results dictionary \n",
    "        results[\"train_loss\"].append(train_loss)\n",
    "        results[\"train_acc\"].append(train_acc)\n",
    "        results[\"test_loss\"].append(test_loss)\n",
    "        results[\"test_acc\"].append(test_acc)\n",
    "\n",
    "    # return the filled results dictionaru \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing going_modular/going_modular/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile going_modular/going_modular/utils.py \n",
    "\"\"\"\n",
    "contains various utility functions for pytorch model training and saving\n",
    "\"\"\"\n",
    "import torch \n",
    "from pathlib import Path \n",
    "\n",
    "def save_model(model: torch.nn.Module, \n",
    "              target_dir: str, \n",
    "              model_name: str):\n",
    "    \"\"\"Saves a pytorch model to a target directory\n",
    "\n",
    "    Args:\n",
    "        model: target pytorch model\n",
    "        target_dir: string of target directory path to store the saved models \n",
    "        model_name: a filename for the saved model. Should be included either \".pth\" or \".pt\" as \n",
    "        the file extension.\n",
    "    \"\"\"\n",
    "    # create target directory \n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # create model save path \n",
    "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model name should end with .pt or .pth\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "    # save the model state_dict()\n",
    "    print(f\"[INFO] Saving model to: {model_save_path}\")\n",
    "    torch.save(obj=model.state_dict(), f=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_setup'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdata_setup\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mmodel_builder\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mutils\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# setup hyperparameters \u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'data_setup'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Trains a pytorch model image classification model using device agnostic code \n",
    "\"\"\"\n",
    "import os \n",
    "import torch \n",
    "import data_setup, engine, model_builder, utils\n",
    "\n",
    "from torchvision import transforms \n",
    "\n",
    "# setup hyperparameters \n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_UNITS = 10 \n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# setup directories \n",
    "train_dir = \"../data/pizza_steak_sushi/train\"\n",
    "test_dir = \"../data/pizza_steak_sushi/test\"\n",
    "\n",
    "# setup device agnostic code \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# create transforms\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# create DataLoaders with help from data_setup.py\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir, \n",
    "    test_dir=test_dir, \n",
    "    transform=data_transform, \n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# create model with help from model_builder.py\n",
    "model = model_builder.TinyVGG(\n",
    "    input_shape=3, \n",
    "    hidden_units=HIDDEN_UNITS, \n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "# set loss and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# start training with help from engine.py \n",
    "engine.train(model=model, \n",
    "            train_dataloader=train_dataloader, \n",
    "            test_dataloader=test_dataloader, \n",
    "            loss_fn=loss_fn, \n",
    "            optimizer=optimizer, \n",
    "            epochs=NUM_EPOCHS, \n",
    "            device=device)\n",
    "        \n",
    "# save the model with help from utils.py\n",
    "utils.save_model(model=model, target_dir=\"models\", model_name=\"05_going_modular_script_mode_tinyvgg_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
